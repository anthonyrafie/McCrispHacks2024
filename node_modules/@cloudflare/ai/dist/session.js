import { readStream, getModelSettings, parseInputs, tensorByName } from "./tools";
class InferenceUpstreamError extends Error {
    constructor(message, httpCode) {
        super(message);
        this.name = "InferenceUpstreamError";
        this.httpCode = httpCode;
    }
}
export class InferenceSession {
    constructor(binding, model, options = {}) {
        this.binding = binding;
        this.model = model;
        this.options = options;
    }
    async run(inputs, options) {
        const jsonInputs = parseInputs(inputs);
        const inferRequest = {
            input: jsonInputs,
            stream: false,
        };
        if (options?.stream) {
            inferRequest.stream = options?.stream;
        }
        const body = JSON.stringify(inferRequest);
        const compressedReadableStream = new Response(body).body.pipeThrough(new CompressionStream("gzip"));
        const fetchOptions = {
            method: "POST",
            body: compressedReadableStream,
            headers: {
                ...(this.options?.extraHeaders || {}),
                "content-encoding": "gzip",
                "cf-consn-sdk-version": "1.0.47",
                "cf-consn-model-id": `${this.options.prefix ? `${this.options.prefix}:` : ""}${this.model}`,
                "cf-consn-routing-model": getModelSettings(this.model, "route") || "default",
            },
        };
        const res = this.options.apiEndpoint
            ? await fetch(this.options.apiEndpoint, fetchOptions)
            : await this.binding.fetch("http://workers-binding.ai/run", fetchOptions);
        if (!res.ok) {
            throw new InferenceUpstreamError(await res.text(), res.status);
        }
        if (!options?.stream) {
            const { result } = await res.json();
            return tensorByName(result);
        }
        else {
            return readStream(res.body, this.options.debug, this.options.ctx, true, options.postProcessing);
        }
    }
}
