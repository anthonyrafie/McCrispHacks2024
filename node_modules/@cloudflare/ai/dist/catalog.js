import { AiTextGeneration } from "./tasks/text-generation";
import { AiTextClassification } from "./tasks/text-classification";
import { AiTextEmbeddings } from "./tasks/text-embeddings";
import { AiTranslation } from "./tasks/translation";
import { AiSpeechRecognition } from "./tasks/speech-recognition";
import { AiImageClassification } from "./tasks/image-classification";
import { AiObjectDetection } from "./tasks/object-detection";
import { AiTextToImage } from "./tasks/text-to-image";
import { AiSentenceSimilarity } from "./tasks/sentence-similarity";
const chatDefaultContext = "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.";
const codeDefaultContext = "Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using   ```:";
export const modelMappings = {
    "text-classification": {
        models: ["@cf/huggingface/distilbert-sst-2-int8"],
        class: AiTextClassification,
        id: "19606750-23ed-4371-aab2-c20349b53a60",
    },
    "text-to-image": {
        models: ["@cf/stabilityai/stable-diffusion-xl-base-1.0"],
        class: AiTextToImage,
        id: "3d6e1f35-341b-4915-a6c8-9a7142a9033a",
    },
    "sentence-similarity": {
        models: ["@hf/sentence-transformers/all-minilm-l6-v2"],
        class: AiSentenceSimilarity,
        id: "69bf4e84-441e-401a-bdfc-256fd54d0fff",
    },
    "text-embeddings": {
        models: [
            "@cf/baai/bge-small-en-v1.5",
            "@cf/baai/bge-base-en-v1.5",
            "@cf/baai/bge-large-en-v1.5",
            "@hf/baai/bge-base-en-v1.5",
        ],
        class: AiTextEmbeddings,
        id: "0137cdcf-162a-4108-94f2-1ca59e8c65ee",
    },
    "speech-recognition": {
        models: ["@cf/openai/whisper"],
        class: AiSpeechRecognition,
        id: "dfce1c48-2a81-462e-a7fd-de97ce985207",
    },
    "image-classification": {
        models: ["@cf/microsoft/resnet-50"],
        class: AiImageClassification,
        id: "00cd182b-bf30-4fc4-8481-84a3ab349657",
    },
    "object-detection": {
        models: ["@cf/facebook/detr-resnet-50"],
        class: AiObjectDetection,
        id: "9c178979-90d9-49d8-9e2c-0f1cf01815d4",
    },
    "text-generation": {
        models: [
            "@cf/meta/llama-2-7b-chat-int8",
            "@cf/mistral/mistral-7b-instruct-v0.1",
            "@cf/meta/llama-2-7b-chat-fp16",
            "@hf/thebloke/llama-2-13b-chat-awq",
            "@hf/thebloke/zephyr-7b-beta-awq",
            "@hf/thebloke/mistral-7b-instruct-v0.1-awq",
            "@hf/thebloke/codellama-7b-instruct-awq",
            "@hf/thebloke/openchat_3.5-awq",
            "@hf/thebloke/openhermes-2.5-mistral-7b-awq",
            "@hf/thebloke/starling-lm-7b-alpha-awq",
            "@hf/thebloke/orca-2-13b-awq",
            "@hf/thebloke/neural-chat-7b-v3-1-awq",
            "@hf/thebloke/llamaguard-7b-awq",
            "@hf/thebloke/deepseek-coder-6.7b-base-awq",
            "@hf/thebloke/deepseek-coder-6.7b-instruct-awq",
        ],
        class: AiTextGeneration,
        id: "c329a1f9-323d-4e91-b2aa-582dd4188d34",
    },
    translation: {
        models: ["@cf/meta/m2m100-1.2b"],
        class: AiTranslation,
        id: "f57d07cb-9087-487a-bbbf-bc3e17fecc4b",
    },
};
const tgiPostProc = (response, ignoreTokens) => {
    let r = response["generated_text"].value[0];
    if (ignoreTokens) {
        for (var i in ignoreTokens)
            r = r.replace(ignoreTokens[i], "");
    }
    return r;
};
export const modelSettings = {
    "@hf/sentence-transformers/all-minilm-l6-v2": {
        experimental: true,
    },
    "@hf/baai/bge-base-en-v1.5": {
        postProcessingFunc: (r) => {
            return {
                shape: r.data.shape,
                data: r.data.value,
            };
        },
    },
    "@hf/thebloke/deepseek-coder-6.7b-instruct-awq": {
        route: "thebloke-deepseek-coder-6-7b-instruct-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "deepseek",
            defaultContext: codeDefaultContext,
        },
        postProcessingFunc: (r) => tgiPostProc(r, ["<|EOT|>"]),
        postProcessingFuncStream: (r) => tgiPostProc(r, ["<|EOT|>"]),
    },
    "@hf/thebloke/deepseek-coder-6.7b-base-awq": {
        route: "thebloke-deepseek-coder-6-7b-base-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "bare",
            defaultContext: codeDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/llamaguard-7b-awq": {
        route: "thebloke-llamaguard-7b-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "inst",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/openchat_3.5-awq": {
        route: "thebloke-openchat-3-5-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "openchat",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/openhermes-2.5-mistral-7b-awq": {
        route: "thebloke-openhermes-2-5-mistral-7b-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "chatml",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: (r) => tgiPostProc(r, ["<|im_end|>"]),
        postProcessingFuncStream: (r) => tgiPostProc(r, ["<|im_end|>"]),
    },
    "@hf/thebloke/starling-lm-7b-alpha-awq": {
        route: "thebloke-starling-lm-7b-alpha-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "openchat",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: (r) => tgiPostProc(r, ["<|end_of_turn|>"]),
        postProcessingFuncStream: (r) => tgiPostProc(r, ["<|end_of_turn|>"]),
    },
    "@hf/thebloke/orca-2-13b-awq": {
        route: "thebloke-orca-2-13b-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "chatml",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/neural-chat-7b-v3-1-awq": {
        route: "thebloke-neural-chat-7b-v3-1-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "orca-hashes",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@cf/stabilityai/stable-diffusion-xl-base-1.0": {
        route: "stable-diffusion-xl-base-1-0",
    },
    "@hf/thebloke/llama-2-13b-chat-awq": {
        route: "thebloke-llama-2-13b-chat-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 512,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/zephyr-7b-beta-awq": {
        route: "thebloke-zephyr-7b-beta-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "zephyr",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/mistral-7b-instruct-v0.1-awq": {
        route: "thebloke-mistral-7b-instruct-v0-1-awq",
        experimental: true,
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "mistral-instruct",
            defaultContext: chatDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@hf/thebloke/codellama-7b-instruct-awq": {
        route: "thebloke-codellama-7b-instruct-awq",
        inputsDefaultsStream: {
            max_tokens: 596,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
            defaultContext: codeDefaultContext,
        },
        postProcessingFunc: tgiPostProc,
        postProcessingFuncStream: tgiPostProc,
    },
    "@cf/meta/llama-2-7b-chat-fp16": {
        route: "llama-2-7b-chat-fp16",
        inputsDefaultsStream: {
            max_tokens: 2500,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
            defaultContext: chatDefaultContext,
        },
    },
    "@cf/meta/llama-2-7b-chat-int8": {
        route: "llama_2_7b_chat_int8",
        inputsDefaultsStream: {
            max_tokens: 1800,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "llama2",
            defaultContext: chatDefaultContext,
        },
    },
    "@cf/openai/whisper": {
        experimental: true,
        postProcessingFunc: (response) => {
            if (response["word_count"]) {
                return {
                    text: response["name"].value.join("").trim(),
                    word_count: parseInt(response["word_count"].value),
                    words: response["name"].value.map((w, i) => {
                        return {
                            word: w.trim(),
                            start: response["timestamps"].value[0][i][0],
                            end: response["timestamps"].value[0][i][1],
                        };
                    }),
                };
            }
            else {
                return {
                    text: response["name"].value.join("").trim(),
                };
            }
        },
    },
    "@cf/mistral/mistral-7b-instruct-v0.1": {
        route: "mistral-7b-instruct-v0-1",
        inputsDefaultsStream: {
            max_tokens: 1800,
        },
        inputsDefaults: {
            max_tokens: 256,
        },
        preProcessingArgs: {
            promptTemplate: "mistral-instruct",
            defaultContext: chatDefaultContext,
        },
    },
};
export const addModel = (task, model, settings) => {
    modelMappings[task].models.push(model);
    modelSettings[model] = settings;
};
